% CHAPTER 7
\chapter{Conclusions and Future Work}
\label{chp:6_conclusion}
\section{Conclusions}
Adopting the techniques used in multimedia compression and using the idea that pixel shifts in a constrained neighborhood are hard to notice, we designed a method that applies local spatial transformations to chrominance channels of perceptual colorspaces. The proposed method results in adversarial images having imperceptible distortions without requiring any regularization term for visual or perceptual quality. In addition to obtaining competitive fooling rates, restricting magnitude of the spatial transformations still yields successful attacks, when there is sufficient amount of local chrominance variation in the input image. As a limitation, this method may produce clipping artifacts which is visible by human observers when the spatial flow application produces out-of-gamut color values, which is also seen in applications of chroma subsampling for lossy visual media compression.


\section{Future Work}
In addition to the perceptual colorspaces investigated in this work, other perceptual colorspaces such as CIELUV, HSLuv and CIEXYZ~\cite{schanda2007colorimetry} can also be utilized to create imperceptible adversarial examples. Out of gamut values at borders with red pixels may result in visible artifacts during the adversarial image generation and preventing such out-of-gamut values would result in better quality adversarial images. To accomplish this, sophisticated projection methods of out-of-gamut pixels towards the boundaries of possible colors can be utilized instead of naively clipping the pixels into the viable range, which produces visible artifacts since clipping pixel values cause changes in luminance component of pixel values. While our method does not require optimizing using a visual quality metric, it can be utilized along with our method to obtain a better visual quality.
As having imperceptible adversarial examples has implications in security and privacy in Artificial Intelligence (AI), data poisoning attacks using imperceptible adversarial examples is a promising direction for AI security and privacy research~\cite{carlini2017towards,ilyas2019adversarial}. Since adversarial robustness research is gaining interest in computer vision research, comparison of our method with the methods of generating imperceptible adversarial examples in current literature against adversarially trained networks remains an open research topic.

Recently, there is a trend in computer vision research towards self-attention based transformer architectures~\cite{vaswani2017attention} for visual classification or detection tasks~\cite{dosovitskiy2020image}. However, their adversarial robustness and behavior on adversarial settings are not yet well understood. Since we only have investigated Convolutional Neural Networks(CNNs), exploring the properties of self attention based vision architectures against our method is on open research area.




