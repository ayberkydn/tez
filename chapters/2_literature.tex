\chapter{related work}
\label{chp:2_literature}

In this chapter, related studies are given in detail. Firstly, the concept of adversarial examples and the types of adversarial examples are explained briefly. Then, spatially transformed adversarial examples are explained in detail.
\section{Adversarial Examples}

The concept of adversarial examples were introduced by Szegedy et al.~\cite{szegedy2013intriguing}. They found that adding small calculated perturbances to the input image is able to change the decision of the target classifier (deep neural network) without affecting the decision human observers. Even though the perturbation is small, it can be easily noticed by human observers as "visual noise" instead of semantically meaningful patterns. A widely used example for adversarial examples is shown in Figure~\ref{fig:advexample}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fgsm_panda_image.png}
    \caption{Visual illustration of adversarial examples. Taken from Goodfellow et al.~\cite{goodfellow2014explaining}}\label{fig:advexample}
\end{figure}


\subsubsection*{White-box and Black-box Attacks}
Adversarial attacks can be classified depending on whether the attacker has access to the neural network parameters and gradients. White-box attacks has access to all the parameters and gradients of the network and adversarial examples are generated by direct optimization. Black-box attacks are generally done by generating adversarial example by attacking a proxy network with white-box transferable attack methods ~\cite{tramer2017space} and use generated examples against the target network.

\subsubsection*{Targeted and Untargeted Attacks}
Adversarial attacks are also classified as targeted or untargeted (evasion) attacks. In a targeted attack setup, the attack is considered successful if the network outputs the particular target class (which is different from a ground-truth label) when fed with the modified adversarial example. On the other hand, an untargeted attack is considered successful if the network outputs any class other than the true class label. This is generally accomplished by selecting a suitable cost function.

Adversarial attacks can be formally defined as a constrained optimization problem where the attacker tries to maximize a loss function by perturbing the input while the magnitude of the perturbation is constrained. Generally, the adversarial loss is the loss that is minimized throughout the training process. In its most general form, the optimization process can be formulated as in Equation \ref{eqn:advattackeqn}. There is often a magnitude constraint to the perturbation and \(\mathcal{L}_p\) norms are widely used to measure the magnitude of the added perturbation.
\begin{equation}
    \label{eqn:advattackeqn}
    \underset{\|\delta\| \leq \epsilon}{\operatorname{maximize}} \ell\left(h_{\theta}(x+\delta), y\right)
\end{equation}

The equation simply indicates that the adversary is aiming to maximize a loss function of the adversarial image and the target label with the norm of the perturbation is upper bounded by \(\epsilon\). In an untargeted attack setup, the function is a single argument function of the adversarial image.

Since naively trained models are vulnerable to adversarial perturbations, the concept of adversarial robustness gained importance and several adversarial robustness methods and benchmarks have been introduced ~\cite{madry2017towards,shafahi2019adversarial,shafahi2018adversarial,tramer2017ensemble,ross2018improving}. In adversarial training, the datapoints are sampled from both the original training set and on-the-fly generated adversarial images. Although the training time significantly increases, since adversarial example generation needs at least one forward and backward pass on the model, this method has been shown to increase the adversarial robustness of the trained model. Also, the concept of universal adversarial perturbations is being investigated after DNNs have been found to be vulnerable to perturbations that is independent of the input image.~\cite{sen2021training}

\section{Adversarial Example Generation Methods}\label{section:methods}
There are several methods for adversarial example generation and the most popular ones are explained in this section. The variables used in this section are defined as follows;
\begin{itemize}
    \setlength\itemsep{0em}
    \item \(x\): Benign image
    \item \(x_{adv}\): Adversarial image
    \item \(y\): Target class
    \item \(J(x, y)\): Cost function with respect to the benign image and the target class
    \item \(Z(x)_i\): Score of $i^{th}$ class on input image \(x\)
\end{itemize}

\subsection{Fast Gradient Sign Method}

Goodfellow et al. found that local linearity of DNNs leads to \(l_\infty\) vulnerability \cite{goodfellow2014explaining} and proposed the Fast Gradient Sign Method~(FGSM) method for generating adversarial examples with \(l_\infty\) norm constraint by using the sign of gradient of the loss with respect to the input. The formal equation of the adversarial image generation process is given in Equation \ref{eq:fgsm}

\begin{equation}
    \label{eq:fgsm}
    x_{adv} = x + \epsilon * \text{sign}(\bigtriangledown_{x}J(x,y))
\end{equation}

According to the authors, this method works even when \(\epsilon\) is too small since the deep neural networks are actually linear in small local epsilon neighborhood of the input point. This method is typically used as a fast and easy to compute baseline in most studies.

After FGSM, many iterative methods have been proposed for adversarial example generation with \(\mathcal{L}_p\) norm constraints. The most prominent and widely used iterative algorithms are Projected Gradient Descent~(PDG) and Carlini \& Wagner method (C\&W).

\subsection{Basic Iterative Method \& Projected Gradient Descent}
Basic Iterative Method~(BIM)~\cite{kurakin2018adversarialphys} can be simply thought as the iterative version of FGSM except that at each iteration, the adversarial example is clipped to the \(\epsilon\) neighborhood of the benign datapoint. This has been found to be a useful heuristic since it allows generating more successful examples in terms of fooling rate without requiring significant computational budget.

Projected Gradient Descent~(PGD) \cite{madry2017towards} is very similar to BIM except that at each iteration projection operation instead of clipping is used to pull the adversarial example back to the \(\mathcal{L}_p\) ball around the original datapoint if necessary so that the \(\mathcal{L}_p\) constraint is satisfied throughout the adversarial example generation process. This method is found to provide better robustness for the models trained with adversarial training than FGSM when training. The iteration steps of PGD and BIM for adversarial example generation are formalized in Equations \ref{eq:bim} and \ref{eq:pgd}, respectively. This process is repeated until the attack is successful or the maximum number of iterations is reached.

\begin{equation}
    \label{eq:bim}
    v^{i+1}=\operatorname{Clip}_{\epsilon}\left\{v^{i}+\alpha \operatorname{sign}\left(\nabla_{x+v^{i}} J\left(x+v^{i}, y\right)\right)\right\}
\end{equation}

\begin{equation}
    \label{eq:pgd}
    v^{i+1}=\operatorname{Project}_{\epsilon}\left\{v^{i}+\alpha \operatorname{sign}\left(\nabla_{x+v^{i}} J\left(x+v^{i}, y\right)\right)\right\}
\end{equation}


\subsection{Carlini \& Wagner Attack}
Carlini \& Wagner~\cite{carlini2017towards} attack is an iterative method for generating adversarial examples. It uses reformulation of the constrained optimization objective by defining a new variable \(w\) so that the constraints are naturally satisfied without requiring an explicit step for clipping or projecting the adversarial example in the \(\mathcal{L}_p\) ball. The optimization process is formulated in Equation~\ref{eq:cw} and Equation~\ref{eq:cw2} where \(\kappa\) denotes the target confidence for the adversarial example, which is the aimed score difference between the target class and the non-target class with the highest score.

\begin{equation}
    \label{eq:cw}
    \operatorname{minimize}\left\|\frac{1}{2}(\tanh (w)+1)-\boldsymbol{I}\right\|_{2}^{2}+c \cdot f\left(\frac{1}{2}(\tanh (w)+1)\right)
\end{equation}

where

\begin{equation}
    \label{eq:cw2}
    F(m)=\max \left(\max _{i \neq t}\left(Z(m)_{i}\right)-Z(m)_{t}, \kappa\right)
\end{equation}



\section{Perceptual Colorspaces}
In most image processing tasks, standard RGB colorspace is used, where R, G, B denotes the Red, Blue, Green components of the image. However, it is incompatible with Human Visual System (HVS) so the need for HVS compatible (perceptual) colorspaces arises for the tasks concerning HVS such as lossy visual media compression where the media is compressed to reduce the size without affecting the perceived output. In this thesis, standard RGB colorspace and the perceptual colorspaces YUV, \(YC_{b}C_{r}\) and CIELAB are used for experiments.

\subsection{YUV and YCbCr}
The \(YC_{b}C_{r}\) model defines a luminance component (Y) and two chrominance components \(C_{b}, C_{r}\) to specify color. \(YC_{b}C_{r}\) is a colorspace that is used in digital photography and visual media compression. In this space, luminance (brightness) and chrominance (color) is separated according to human visual perception. Y dimension of the space is the luminance information, or simply a grayscale representation of the image. \(C_b\) and \(C_r\) dimensions are the blue-difference and red-difference chroma components, respectively. The relation between RGB space and \(YC_{b}C_{r}\) space is modeled as Equation~\ref*{eq:rgbycbcr}, which is a set of linear equations defined in ITU-T H.273~\cite{hamilton2004jpeg};

\begin{align}
    \label{eq:rgbycbcr}
    \begin{split}
        Y   & = 0.299 R+0.587 G+0.114 B                   \\
        C_b & = 128-(0.168736 R)-(0.331264 G)+(0.5 B)     \\
        C_r & = 128+(0.5 R)-(0.418688 G)-(0.081312 B)     \\
        R & = Y+1.402(C_{r}-128)                        \\
        G & = Y-0.344136(C_{b}-128)-0.714136(C_{r}-128) \\
        B & = Y+1.772C_b-128
    \end{split}
\end{align}

YUV is often considered as the analog counterpart of \(YC_{b}C_{r}\). There is no significant difference between YUV and \(YC_{b}C_{r}\) except for the scaling of chrominance components to the range 0-255 for \(YC_{b}C_{r}\) so that they can be represented as unsigned integers while chrominance values can be negative in YUV colorspace. For that reason, these two terms are often used interchangeably.

\subsection{CIEXYZ and CIELAB}
CIE 1931 XYZ (CIEXYZ) colorspace was proposed by International Commission of Illumination (CIE) in 1931 after a series of human experiments on color perception. There are three components, namely X, Y and Z. While Y denotes the luma component, there is no independent representations of XZ components and these components together represent possible chrominance values for given Y value.
CIELAB colorspace~\cite{schanda2007colorimetry} defined by the International Commission on Illumination (CIE) has the following three components: L, \(a^*\) and \(b^*\). L is perceptual lightness where \(L = 0\) and \(L* = 100\) define a black and a white pixel, respectively, regardless of the \(a^*\) and \(b^*\) values. \(a^*\) and \(b^*\) dimensions are the chroma components. They are designed to be perceptually uniform where a numerical change in pixel value corresponds to a similar change in human perception ~\cite{mahy1992luminancevschroma}. Both chroma components are in the range \([-127, 127]\). Unlike \(YC_{b}C_{r}\), CIELAB space does not have a linear relationship with RGB space. In fact, conversion to an intermediary space CIEXYZ is needed to transform from RGB to CIELAB and there are different implementations of CIELAB conversion. We used the RGB to CIELAB implementation from Kornia library~\cite{riba2020kornia}, which assumes D65 illuminant and Observer 2.

\section{Perceptual Distance Metrics}
There is an ongoing research on finding difference metrics over 2D images that aligns with human visual perception, which is challenging due to the nature and lack of knowledge about the human vision. \cite{ding2021comparison}. There are several studies proposing perceptual metrics with different methods. The perceptual metrics used in this thesis is briefly explained.

\subsection{Structural Similarity Index Measure (SSIM)}
Structural Similarity Index Measure is a method for measuring similarity between two images. Although it is a naive method for measuring difference between two images, it is considered to be a perceptual difference metric since it takes structural information into account. SSIM calculates 3 different comparison measures (luminance, contrast and structure) between two images $x$ and $y$, which is shown in Equation~\ref{eq:ssim1} where $l$ denotes luminance, $c$ denotes contrast and $s$ denotes structure measure.

\begin{equation}
    \label{eq:ssim1}
    \begin{aligned}
         & l(x, y)=\frac{2 \mu_{: x} \mu_{y}+c_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+c_{1}}           \\
         & c(x, y)=\frac{2 \sigma_{x} \sigma_{y}+c_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+c_{3}} \\
         & s(x, y)=\frac{\sigma_{x y}+c_{2}/2}{\sigma_{x} \sigma_{y}+c_{2}/2}
    \end{aligned}
\end{equation}

with
\begin{itemize}
    \setlength\itemsep{0em}
    \item \(\mu_x\): mean of \(x\)
    \item \(\mu_y\): mean of \(y\)
    \item \(\sigma_x\): standard deviation of \(x\)
    \item \(\sigma_y\): standard deviation of \(y\)
    \item \(\sigma_{xy}\): covariance of \(x\) and \(y\)
    \item \(L\): \(2^{\#bits per pixel}-1\)
    \item \(c_1\): \(0.0001 \times L^2\)
    \item \(c_2\): \(0.0009 \times L^2\)
\end{itemize}

Using these three measures, SSIM is calculated according to Equation~\ref{eq:ssim2}
\begin{equation}
    \label{eq:ssim2}
    SSIM(x,y) = l(x,y) \times c(x,y) \times s(x,y)
\end{equation}

Although there are many variations of SSIM, original SSIM and Multi-Scale SSIM (MS-SSIM) are used in this thesis. The difference between SSIM and MS-SSIM is MS-SSIM is computed over multiple scales of the compared images through multiple scales of subsampling.

\subsection{Learned Perceptual Image Patch Similarity (LPIPS)}
Zhang et al. found that DNN representations of images are very effective for approximating perceptual similarity of two images, as supported by human experiments~\cite{zhang2018unreasonable}. They proposed Learned Perceptual Image Patch Similarity to assess perceptual distance between images. It works by simply computing the Euclidean distance between deep representations of trained deep convolutional networks. They experimented with different DNN architectures and have not observed significant difference. In this thesis, ImageNet trained VGG-16 is used for LPIPS computation.

\section{Perceptual Quality Preserving Adversarial Attacks}

Utilizing perceptual colorspaces and metrics for imperceptible adversarial example generation is investigated in several studies. Aksoy et al. investigated additive noise based attacks on chrominance channels in YUV colorspace \cite{aksoy2019attack}, which is the analog counterpart of \(YC_{b}C_{r}\) space. Despite Pestana et al. found that adversarial perturbations are more highlighted in luminance channels in terms of the magnitude~\cite{Pestana2020-hm}, Aksoy et al. found that even suppressing the luminance perturbation, additive noise based attack on chrominance channels still successfully fool target networks, yet causes visible distortion. In our earlier work, we also explored spatial transformations to UV channels of YUV to generate imperceptible adversarial examples~\cite{aydin2019imperceptible} and we extend this work by exploring \(YC_{b}C_{r}\) space as well as perceptually uniform CIELAB space and measuring structured similarity metrics such as SSIM~\cite{wang2004image} and MS-SSIM~\cite{wang2003multiscale} between benign images and adversarially generated images. Karli et al. leveraged perceptual metric LPIPS~\cite{zhang2018unreasonable} to improve the quality of adversarial examples. Since LPIPS is a differentiable metric, they used gradient based optimization to minimize LPIPS alongside the adversarial loss. Similarly, Zhao et al.~\cite{zhao2020large} replaced CIEDE2000 perceptual distance metric~\cite{luo2001development} with \(\mathcal{L}_{p}\) norm constraint in Carlini \& Wagner attack to produce perceptually close adversarial examples.

Croce et al. argued adding noise to smooth areas of an image causes visible artifacts and proposed "hiding" the perturbations at the locations with high spatial variations such as edges and corners~\cite{croce2019sparse}. As seen in Figure \ref{fig:diff}, perturbations generated with our method naturally occurs in the places with high variations since it is based on local spatial transforms. Also, since the differences made with our methods affect only the chrominance channels, visualizing these differences apart from luminance component still yields barely noticeable changes while full RGB flow difference is significantly noticeable when visualized on its own. Similarly, Karli et al. proposed increasing the intensity of adversarial perturbation in the regions that have high spatial variance such as edges and corners and attenuating the amount of the overall perturbation by minimizing LPIPS distance between benign and adversarial image. Since LPIPS is differentiable, the minimization can be modeled as an optimization step or an extra term in the cost function.

Unlike these methods, the attack proposed in this paper does not rely on auxiliary losses or explicit perceptual distance terms in optimization process to produce examples with high perceptual quality. In addition, it does not require regularization, unlike spatial transformation based methods such as ~\cite{xiao2018spatially}, due to its intrinsic imperceptibility. It should be noted that the existing spatial transformation based methods, as well as our work, does not utilize limited degree of freedom transformations such as rotation, translation or scaling that can be formulated as a \(4\times4\) transformation matrix~\cite{jaderberg2015spatial}. In that formulation, the flow field \(f \in \mathbb{R}^{2\times H \times W}\) is calculated using the transformation matrix. Instead, we directly define and optimize flow field, where the number of parameters is equal to twice number of pixels in the input image since there is an \(x\) and \(y\) component for each pixel.
%%%%%

\subsection{Spatially Transformed Adversarial Examples}

Spatial transformations as a method for generating adversarial examples was first proposed in ~\cite{xiao2018spatially}, where it is shown that small displacements applied to input pixels can successfully fool a target network. However, using this method, even small displacements could cause visible distortions when the adjacent pixels drift towards different directions. As a remedy to this problem, use of Total Variation~(TV) regularization~\cite{estrela2016total} was proposed. Application of TV regularization to the flow field pushes the neighboring displacement vectors to the same direction and, hence, produces smoother output. Similarly, Jordan et al.~\cite{jordan2019quantifying} combined spatial transformations with \(l_\infty\) bounded attacks to forge stronger attacks with better perceptual quality.

Spatial transformations aim to alter the geometry of the input image instead of changing the pixel values. To accomplish that, Spatially Transformed Adversarial Examples~(stAdv) applies a flow field \(f \in \mathbb{R}^{2\times H \times W}\) whose elements are flow vectors (or displacement vectors) \(f_i\) for each pixel in the adversarial image. Since the elements of displacement vectors are not integers, a need for interpolation to sample fractional positions arises. In this work, bilinear interpolation is used since it is computationally efficient. The application of flow field to the benign image is formulated in Equation \ref{eq:stadv} where \(\mathbf{x}_{\mathrm{adv}}^{(i)}\) denotes the value of $i^{th}$ pixel in the adversarial image and \(u_{adv}\),\(v_{adv}\) denotes the position of that pixel in the adversarial image.
\begin{equation}
    \label{eq:stadv}
    \mathbf{x}_{\mathrm{adv}}^{(i)}=\sum_{g \in \mathcal{N}\left(u^{(i)}, v^{(i)}\right\rangle} \mathbf{x}^{(q)}\left(1-\left|u^{(i)}-u^{(q)}\right|\right)\left(1-\left|v^{(i)}-v^{(q)}\right|\right)
\end{equation}

Since bilinear interpolation is differentiable, application of the flow field is also a differentiable operation and can be optimized by gradient based optimization methods. Visual interpretation of flow field application is shown in Figure \ref{fig:stadv}

\begin{figure}[h]
    \includegraphics[width=\linewidth]{stadv.png}
    \caption{Visual representation of application of flow field to generate spatially transformed example. Taken from Xiao et al. \cite{xiao2018spatially}}\label{fig:stadv}

\end{figure}

Carlini \& Wagner loss in Equation~\ref{eq:cw2} with a Total Variation regularization term \(\mathcal{L}_{flow}\) for flow field \(f\) to reduce the high frequency pixel drift distortion is minimized for adversarial optimization. The TV loss term is shown in Equation~\ref{eq:tv}. The optimization is made with L-BFGS~\cite{liu1989limited} with linear backtracking, however the authors have stated that Adam~\cite{kingma2015adam} optimizer could be used as well.


\begin{equation}
    \label{eq:tv}
    \mathcal{L}_{\text {flow }}(f)=\sum_{p}^{\text {all pixels }} \sum_{q \in \mathcal{N}(p)} \sqrt{\left\|\Delta u^{(p)}-\Delta u^{(q)}\right\|_{2}^{2}+\left\|\Delta v^{(p)}-\Delta v^{(q)}\right\|_{2}^{2}}
\end{equation}





